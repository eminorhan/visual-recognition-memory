Namespace(data_path='/scratch/work/public/imagenet/train', vqconfig_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.yaml', vqmodel_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.ckpt', num_workers=4, seed=0, save_dir='/scratch/eo41/visual-recognition-memory/gpt_pretrained_models', n_layer=48, n_head=25, n_embd=1600, vocab_size=16384, block_size=255, batch_size=24, print_freq=100, lr=0.0001, optimizer='Adam', resume='', gpu=None, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1)
Namespace(data_path='/scratch/work/public/imagenet/train', vqconfig_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.yaml', vqmodel_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.ckpt', num_workers=4, seed=0, save_dir='/scratch/eo41/visual-recognition-memory/gpt_pretrained_models', n_layer=48, n_head=25, n_embd=1600, vocab_size=16384, block_size=255, batch_size=24, print_freq=100, lr=0.0001, optimizer='Adam', resume='', gpu=None, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1)
Namespace(data_path='/scratch/work/public/imagenet/train', vqconfig_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.yaml', vqmodel_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.ckpt', num_workers=4, seed=0, save_dir='/scratch/eo41/visual-recognition-memory/gpt_pretrained_models', n_layer=48, n_head=25, n_embd=1600, vocab_size=16384, block_size=255, batch_size=24, print_freq=100, lr=0.0001, optimizer='Adam', resume='', gpu=None, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1)
Running on 3 GPUs total
model:
  base_learning_rate: 4.5e-06
  params:
    ddconfig:
      attn_resolutions:
      - 16
      ch: 128
      ch_mult:
      - 1
      - 1
      - 2
      - 2
      - 4
      double_z: false
      dropout: 0.0
      in_channels: 3
      num_res_blocks: 2
      out_ch: 3
      resolution: 256
      z_channels: 256
    embed_dim: 256
    lossconfig:
      params:
        codebook_weight: 1.0
        disc_conditional: false
        disc_in_channels: 3
        disc_num_layers: 2
        disc_start: 0
        disc_weight: 0.75
      target: vqloss.VQLPIPSWithDiscriminator
    monitor: val/rec_loss
    n_embed: 16384
  target: vqmodel.VQModel

Working with z of shape (1, 256, 16, 16) = 65536 dimensions.
loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth
VQLPIPSWithDiscriminator running with hinge loss.
Loaded VQ encoder.
Data loaded: dataset contains 1281167 images, and takes 17794 training iterations per epoch.
Number of parameters: 1528398400
=> no checkpoint loaded, will train from scratch
/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Iteration: 0 | Training loss: 10.057271003723145
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_0_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 100 | Training loss: 6.977696518898011
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_100_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 200 | Training loss: 6.714035377502442
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_200_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 300 | Training loss: 6.653052296638489
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_300_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 400 | Training loss: 6.617666120529175
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_400_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 500 | Training loss: 6.565917057991028
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_500_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 600 | Training loss: 6.5326585721969606
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_600_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 700 | Training loss: 6.505565748214722
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_700_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 800 | Training loss: 6.4777878522872925
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_800_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 900 | Training loss: 6.467179932594299
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_900_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 1000 | Training loss: 6.435473971366882
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_1000_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 1100 | Training loss: 6.431002283096314
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_1100_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 1200 | Training loss: 6.414239692687988
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_1200_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 1300 | Training loss: 6.3943515968322755
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_1300_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
Iteration: 1400 | Training loss: 6.3703988790512085
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_1400_48l_25h_1600e_72b_0.0001lr_Adamo_0s.pt
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 24331867 ON ga001 CANCELLED AT 2022-09-01T03:20:16 ***
slurmstepd: error: *** STEP 24331867.0 ON ga001 CANCELLED AT 2022-09-01T03:20:16 ***
