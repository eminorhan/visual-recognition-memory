Namespace(data_path='/scratch/work/public/imagenet/train', vqconfig_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.yaml', vqmodel_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.ckpt', num_workers=8, seed=2, save_dir='/scratch/eo41/visual-recognition-memory/gpt_pretrained_models', n_layer=36, n_head=20, n_embd=1280, vocab_size=16384, block_size=255, batch_size=48, print_freq=5000, lr=0.0003, optimizer='Adam', resume='/scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_155000_36l_20h_1280e_192b_0.0003lr_Adamo_1s.pt', gpu=None, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1)
Namespace(data_path='/scratch/work/public/imagenet/train', vqconfig_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.yaml', vqmodel_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.ckpt', num_workers=8, seed=2, save_dir='/scratch/eo41/visual-recognition-memory/gpt_pretrained_models', n_layer=36, n_head=20, n_embd=1280, vocab_size=16384, block_size=255, batch_size=48, print_freq=5000, lr=0.0003, optimizer='Adam', resume='/scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_155000_36l_20h_1280e_192b_0.0003lr_Adamo_1s.pt', gpu=None, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1)
Namespace(data_path='/scratch/work/public/imagenet/train', vqconfig_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.yaml', vqmodel_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.ckpt', num_workers=8, seed=2, save_dir='/scratch/eo41/visual-recognition-memory/gpt_pretrained_models', n_layer=36, n_head=20, n_embd=1280, vocab_size=16384, block_size=255, batch_size=48, print_freq=5000, lr=0.0003, optimizer='Adam', resume='/scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_155000_36l_20h_1280e_192b_0.0003lr_Adamo_1s.pt', gpu=None, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1)
Namespace(data_path='/scratch/work/public/imagenet/train', vqconfig_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.yaml', vqmodel_path='/scratch/eo41/visual-recognition-memory/vqgan_pretrained_models/imagenet_16x16_16384.ckpt', num_workers=8, seed=2, save_dir='/scratch/eo41/visual-recognition-memory/gpt_pretrained_models', n_layer=36, n_head=20, n_embd=1280, vocab_size=16384, block_size=255, batch_size=48, print_freq=5000, lr=0.0003, optimizer='Adam', resume='/scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_155000_36l_20h_1280e_192b_0.0003lr_Adamo_1s.pt', gpu=None, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1)
Running on 4 GPUs total
model:
  base_learning_rate: 4.5e-06
  params:
    ddconfig:
      attn_resolutions:
      - 16
      ch: 128
      ch_mult:
      - 1
      - 1
      - 2
      - 2
      - 4
      double_z: false
      dropout: 0.0
      in_channels: 3
      num_res_blocks: 2
      out_ch: 3
      resolution: 256
      z_channels: 256
    embed_dim: 256
    lossconfig:
      params:
        codebook_weight: 1.0
        disc_conditional: false
        disc_in_channels: 3
        disc_num_layers: 2
        disc_start: 0
        disc_weight: 0.75
      target: vqloss.VQLPIPSWithDiscriminator
    monitor: val/rec_loss
    n_embed: 16384
  target: vqmodel.VQModel

Working with z of shape (1, 256, 16, 16) = 65536 dimensions.
loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth
VQLPIPSWithDiscriminator running with hinge loss.
Loaded VQ encoder.
Data loaded: dataset contains 1281167 images, and takes 6673 training iterations per epoch.
Number of parameters: 750659840
=> loaded model weights and optimizer state at checkpoint '/scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_155000_36l_20h_1280e_192b_0.0003lr_Adamo_1s.pt'
/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Iteration: 0 | Training loss: 5.296093940734863
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_0_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 5000 | Training loss: 5.292020198154449
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_5000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 10000 | Training loss: 5.2912790122032165
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_10000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 15000 | Training loss: 5.291495257091523
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_15000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 20000 | Training loss: 5.289704079151154
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_20000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 25000 | Training loss: 5.284678612327576
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_25000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 30000 | Training loss: 5.283924589157104
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_30000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 35000 | Training loss: 5.28347427816391
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_35000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 40000 | Training loss: 5.281659729957581
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_40000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 45000 | Training loss: 5.278129335212707
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_45000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 50000 | Training loss: 5.276590184593201
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_50000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 55000 | Training loss: 5.276226564884186
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_55000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 60000 | Training loss: 5.274806654071808
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_60000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 65000 | Training loss: 5.561333432865143
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_65000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 70000 | Training loss: 5.2994397356987
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_70000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 75000 | Training loss: 5.378830035400391
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_75000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
Iteration: 80000 | Training loss: 5.489332132339477
Saving model to: /scratch/eo41/visual-recognition-memory/gpt_pretrained_models/model_80000_36l_20h_1280e_192b_0.0003lr_Adamo_2s.pt
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 24716198 ON ga001 CANCELLED AT 2022-09-12T09:50:29 ***
slurmstepd: error: *** STEP 24716198.0 ON ga001 CANCELLED AT 2022-09-12T09:50:29 ***
